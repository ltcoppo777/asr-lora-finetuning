# Whisper Fine-tuning for Children's Speech Recognition

## Overview
Fine-tuning OpenAI's Whisper-base model using LoRA for improved 
performance on children's speech data.

## Results Summary  
- **Baseline Whisper WER**: 118.82%
- **2 Epoch Fine-tuned Whisper WER**: 58.13%
- **Improvement**: 60.69% improvement
- **Dataset**: 6,923 training samples each epoch, 1,578 test samples

## Download Links
- **Children's Speech Dataset**: [Download here](https://drive.google.com/file/d/1rbnQ2RFLgKBNXqzCp-EHEAh8Z5eKn_r7/view)

*Note: Download the dataset and extract to `./data/` directory, then follow the usage instructions below to train your own model.*

## Setup

### Requirements
- Python 3.10+
- CUDA-compatible GPU (recommended)
- ~13GB available storage 

# Installation
## Clone repository
Clone the repository:
```bash
git clone https://github.com/ltcoppo777/asr-lora-finetuning.git
cd asr-lora-finetuning
```
## Create virtual environment
In your terminal:
```bash
python -m venv venv
venv\Scripts\activate 
```

## Install dependencies
In your terminal:
```bash
pip install -r requirements.txt 
```

## Download Required Files
1. Download the children's speech dataset from the link above
2. Extract to `./data/` directory
3. Follow the usage instructions to train and evaluate your own model

# File Structure
```
asr-lora-finetuning/
├── config/
│   └── config.py              # Configuration settings
├── filelists/
│   ├── train/
│   │   └── train.csv          # Training data manifest
│   └── test/
│       └── test.csv           # Test data manifest
├── scripts/
│   ├── count_data.py          # Data counting utility
│   ├── data_prep.py           # Audio segmentation script
│   ├── evaluate_models.py     # Model evaluation script
│   └── train_whisper_lora.py  # LoRA fine-tuning script
├── .gitignore
├── README.md
└── requirements.txt

# After downloading:
├── fine_tuned_whisper/        # Your trained model (created after training)
├── data/                      # Downloaded children's speech dataset
└── processed_data/            # Generated by data_prep.py
```

# Configuration

All settings are controlled through `config/config.py`. Key parameters you can modify:

### Path Configuration
```python
#These paths are set automatically based on project structure:
TRAIN_PATH = os.path.join(ROOT_DIR, "data", "data", "train")
TEST_PATH = os.path.join(ROOT_DIR, "data", "data", "test") 
FILELIST_PATH = os.path.join(ROOT_DIR, "filelists")
PROCESSED_DATA_PATH = os.path.join(ROOT_DIR, "processed_data")
```
### Training Settings
```python
N_SAMPLES = 6923              # Number of training samples (6923 = full dataset)
NUM_TRAINING_STEPS = 6293     # Training iterations
LEARNING_RATE = 1e-4          # Learning rate for fine-tuning
PRINT_EVERY_X_STEPS = 100     # Progress update frequency
```

### LoRA Configuration
```python
LORA_RANK = 16                # LoRA adaptation rank
LORA_ALPHA = 32               # LoRA scaling parameter
LORA_DROPOUT = 0.1            # LoRA dropout rate
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "out_proj"]  # Attention modules
```
### Audio Processing
```python
TRAIN_PAD = 0.5               # Padding for training audio (seconds)
TRAIN_VOLUME = 2.0            # Volume adjustment for training
TEST_PAD = 0.0                # Padding for test audio
TEST_VOLUME = 1.0             # Volume adjustment for test
```

### Evaluation Settings
```python
# Change this line to switch between models:
EVAL_MODEL_PATH = "openai/whisper-base"      # For baseline evaluation
# EVAL_MODEL_PATH = "./fine_tuned_whisper"   # For fine-tuned evaluation
```

# Usage

## Quick Evaluation (Recommended)
To reproduce the results by training your own model:

1. **Download the dataset** from the link above
2. **Extract to `./data/`**
3. **Follow the full pipeline below**

## Full Pipeline (Train Your Own Model)

### Step 1: Data Preparation
```bash
python scripts/data_prep.py
```
Segments raw audio files based on timestamps and creates training/test CSV manifests.

### Step 2: Training
```bash
python scripts/train_whisper_lora.py
```
Fine-tunes Whisper-base using LoRA on children's speech data. Training progress will be displayed every 100 steps. Model saves to ./fine_tuned_whisper/ upon completion.

### Step 3: Evaluation
Evaluate baseline model:
```bash
python scripts/evaluate_models.py
```
**(MAKE SURE: EVAL_MODEL_PATH = "openai/whisper-base" in config.py)**

Evaluate fine-tuned model:
```bash
python scripts/evaluate_models.py
```
**(Change EVAL_MODEL_PATH = "./fine_tuned_whisper" in config.py)**

### Expected Output
- Training displays loss reduction over ~6293 steps (editable in config)
- Evaluation shows WER results for 1578 test samples (editable in config)
- Results summary displays average WER, best/worst cases

## Training Details
- **Architecture**: Whisper-base (74M parameters) with LoRA adapters
- **Trainable parameters**: 1,179,648 (1.6% of total model)
- **Training strategy**: Single-sample gradient accumulation 
- **Memory optimization**: Mixed precision, gradient checkpointing
- **Training time**: ~15-30 minutes on modern laptop GPU (5070ti Laptop Version)

## Results
Evaluated on 1,578 test samples:

| Model | Average WER | Best WER | Worst WER |
|-------|-------------|----------|-----------|
| Baseline Whisper | 118.82% | 0.00% | 111.00% |
| Fine-tuned-2Epochs | 58.13% | 0.00% | 88.00% |

**Key Finding**: Training for 2 epochs as well as the LoRA configurations provided substantial 
improvement, showing the value of seeing training data multiple times.

## Troubleshooting
- **CUDA out of memory**: Reduce batch size or use CPU evaluation
- **Missing model files**: Ensure fine-tuned model is downloaded and extracted correctly
- **Path errors**: Check that file paths in config.py match your directory structure

## License
MIT License - see LICENSE file for details.
